\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{samuel_studies_1959}
\citation{krizhevsky_imagenet_2012,simonyan_very_2014,szegedy_going_2015,he_deep_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Research context}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definitions and notation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Image representation}{2}}
\newlabel{eq_2d_matrix}{{2.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces RGB image with 4 pixels represented as a 3 dimensional matrix\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:image_matrix}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Gradient}{3}}
\newlabel{se:gradient}{{2.1.2}{3}}
\newlabel{eq:gradient}{{2.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Activation functions}{3}}
\newlabel{se:activation_functions}{{2.1.3}{3}}
\newlabel{eq:sigmoid}{{2.3}{3}}
\newlabel{eq:tanh}{{2.4}{3}}
\newlabel{eq:relu}{{2.5}{3}}
\newlabel{fig:sigmoid}{{2.2a}{4}}
\newlabel{sub@fig:sigmoid}{{a}{4}}
\newlabel{fig:tanh}{{2.2b}{4}}
\newlabel{sub@fig:tanh}{{b}{4}}
\newlabel{fig:relu}{{2.2c}{4}}
\newlabel{sub@fig:relu}{{c}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Nonliear activation functions\relax }}{4}}
\newlabel{fig:subfig1.a.4}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Metrics}{4}}
\newlabel{eq:accuracy}{{2.6}{4}}
\newlabel{eq:precision}{{2.7}{4}}
\newlabel{eq:recall}{{2.8}{4}}
\newlabel{eq:f1score}{{2.9}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Confusion matrix\relax }}{5}}
\newlabel{tb:confusion_matrix}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Data}{5}}
\newlabel{se:data}{{2.1.5}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine learning}{5}}
\newlabel{se:machine_learning}{{2.2}{5}}
\newlabel{eq:hypotesis}{{2.10}{5}}
\newlabel{eq:model}{{2.11}{6}}
\newlabel{eq:error_function}{{2.12}{6}}
\newlabel{eq:optimization_function}{{2.13}{6}}
\newlabel{eq:regression_def}{{2.14}{6}}
\newlabel{eq:classification_def}{{2.15}{6}}
\citation{srivastava_dropout:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Performance of a model with time using training and test set\relax }}{7}}
\newlabel{fig:overfitting}{{2.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Supervised and unsupervised learning}{7}}
\citation{mcculloch_logical_1943}
\citation{kleene_representation_1951}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Artificial neural network (ANN)}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Neuron cell in a biological brain.\relax }}{9}}
\newlabel{fig:neuron}{{2.4}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Artificial neuron cell.\relax }}{9}}
\newlabel{fig:ann_neuron}{{2.5}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Feed-forward neural network\relax }}{10}}
\newlabel{fig:ann}{{2.6}{10}}
\newlabel{eq:neuron_net_output}{{2.16}{10}}
\newlabel{eq:neuron_output}{{2.17}{10}}
\citation{hubel_receptive_1968}
\citation{fukushima_neocognitron:_1982,eckmiller_hierarchical_1989}
\citation{krizhevsky_imagenet_2012,simonyan_very_2014,szegedy_going_2015,he_deep_2016}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep learning}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Convolutional Neural Network (CNN / ConvNet)}{11}}
\citation{simonyan_very_2014}
\citation{simonyan_very_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces 3D structure of a convolutional filter (on the left painted in pink is the input and the blue rectangle with circle is a filter)\relax }}{12}}
\newlabel{fig:conv}{{2.7}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A 3x3 kernel (filter) convolving and input to generate an output\relax }}{13}}
\newlabel{fig:conv1}{{2.8}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Representation of filters through the CNN\relax }}{13}}
\newlabel{fig:filter}{{2.9}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Max pooling a 4x4 input to 2x2\relax }}{13}}
\newlabel{fig:pool}{{2.10}{13}}
\citation{srivastava_dropout:_2014}
\citation{ioffe_batch_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical modern CNN\relax }}{14}}
\newlabel{fig:cnn}{{2.11}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces An example of the dropout technique\relax }}{14}}
\newlabel{fig:dropout}{{2.12}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Dropout}{14}}
\newlabel{se:dropout}{{2.3.2}{14}}
\citation{kingma_adam:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces An example of the original image 224x224x3 in $X_{train}$\relax }}{15}}
\newlabel{fig:data_augmentation_figure}{{2.13}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Data Augmentation}{15}}
\newlabel{se:data_augmentation}{{2.3.3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Backpropagation}{15}}
\newlabel{se:backprop}{{2.3.4}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Descent}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Example of data augmented images based on the original image \ref  {fig:data_augmentation_figure}\relax }}{16}}
\newlabel{fig:data_augmentation}{{2.14}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces An example of local and global maxima and minima in a function\relax }}{17}}
\newlabel{fig:local_and_global_function_values}{{2.15}{17}}
\newlabel{eq:gradient_descent}{{2.18}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{17}}
\newlabel{se:stochastic_gd}{{2.3.4}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces An example of gradient descent converging to global minima in a 2D space (2 variable function)\relax }}{18}}
\newlabel{fig:local_and_global_function_values}{{2.16}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Optimization Target}{18}}
\newlabel{eq:softmax}{{2.19}{18}}
\citation{clevert_fast_2015,xu_empirical_2015,he_delving_2015}
\citation{he_deep_2016}
\citation{ioffe_batch_2015}
\newlabel{eq:cross_entropy_loss}{{2.20}{19}}
\newlabel{eq:cross_entropy_graidnet}{{2.21}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Vanishing Gradient}{19}}
\newlabel{se:vanishing_gradient}{{2.3.5}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Batch Normalization}{19}}
\newlabel{se:batch_norm}{{2.3.6}{19}}
\citation{he_deep_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Model}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Implementation}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Keras}{21}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Keras high level API example}{22}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Dataset}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}ImageNet}{23}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{24}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{fer}
\bibdata{master_thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{25}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{_tensorflow_????}{{1}{}{{_te}}{{}}}
\bibcite{clevert_fast_2015}{{2}{}{{Clevert et~al.}}{{Clevert, Unterthiner, i Hochreiter}}}
\bibcite{eckmiller_hierarchical_1989}{{3}{}{{Fukushima}}{{}}}
\bibcite{fukushima_neocognitron:_1982}{{4}{}{{Fukushima i Miyake}}{{}}}
\bibcite{he_deep_2016}{{5}{a}{{He et~al.}}{{He, Zhang, Ren, i Sun}}}
\bibcite{he_delving_2015}{{6}{b}{{He et~al.}}{{He, Zhang, Ren, i Sun}}}
\bibcite{hubel_receptive_1968}{{7}{}{{Hubel i Wiesel}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{26}}
\bibcite{ioffe_batch_2015}{{8}{}{{Ioffe i Szegedy}}{{}}}
\bibcite{kingma_adam:_2014}{{9}{}{{Kingma i Ba}}{{}}}
\bibcite{kleene_representation_1951}{{10}{}{{Kleene}}{{}}}
\bibcite{krizhevsky_imagenet_2012}{{11}{}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, i Hinton}}}
\bibcite{mcculloch_logical_1943}{{12}{}{{{McCulloch} i Pitts}}{{}}}
\bibcite{samuel_studies_1959}{{13}{}{{Samuel}}{{}}}
\bibcite{simonyan_very_2014}{{14}{}{{Simonyan i Zisserman}}{{}}}
\bibcite{srivastava_dropout:_2014}{{15}{}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, i Salakhutdinov}}}
\bibcite{szegedy_going_2015}{{16}{}{{Szegedy et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, i Rabinovich}}}
\bibcite{xu_empirical_2015}{{17}{}{{Xu et~al.}}{{Xu, Wang, Chen, i Li}}}
