\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{how_many_species_2011}
\citation{samuel_studies_1959}
\citation{cifar}
\citation{imagenet}
\citation{kaggle}
\citation{krizhevsky_imagenet_2012,simonyan_very_2014,szegedy_going_2015,he_deep_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{se:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Research context}{2}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definitions and notation}{2}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Image representation}{2}{subsection.2.1.1}}
\newlabel{eq_2d_matrix}{{2.1}{2}{Image representation}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces RGB image with 9 pixels represented as a 3-dimensional matrix\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:image_matrix}{{2.1}{3}{RGB image with 9 pixels represented as a 3-dimensional matrix\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Gradient}{3}{subsection.2.1.2}}
\newlabel{se:gradient}{{2.1.2}{3}{Gradient}{subsection.2.1.2}{}}
\newlabel{eq:gradient}{{2.2}{3}{Gradient}{equation.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Activation functions}{3}{subsection.2.1.3}}
\newlabel{se:activation_functions}{{2.1.3}{3}{Activation functions}{subsection.2.1.3}{}}
\newlabel{eq:sigmoid}{{2.3}{3}{Activation functions}{equation.2.1.3}{}}
\newlabel{eq:tanh}{{2.4}{3}{Activation functions}{equation.2.1.4}{}}
\newlabel{fig:sigmoid}{{2.2a}{4}{$simgoid$\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sigmoid}{{a}{4}{$simgoid$\relax }{figure.caption.3}{}}
\newlabel{fig:tanh}{{2.2b}{4}{$tanh$\relax }{figure.caption.3}{}}
\newlabel{sub@fig:tanh}{{b}{4}{$tanh$\relax }{figure.caption.3}{}}
\newlabel{fig:relu}{{2.2c}{4}{$relu$\relax }{figure.caption.3}{}}
\newlabel{sub@fig:relu}{{c}{4}{$relu$\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Non-linear activation functions\relax }}{4}{figure.caption.3}}
\newlabel{fig:subfig1.a.4}{{2.2}{4}{Non-linear activation functions\relax }{figure.caption.3}{}}
\newlabel{eq:relu}{{2.5}{4}{Activation functions}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Metrics}{4}{subsection.2.1.4}}
\newlabel{se:metrics}{{2.1.4}{4}{Metrics}{subsection.2.1.4}{}}
\newlabel{eq:accuracy}{{2.6}{4}{Metrics}{equation.2.1.6}{}}
\newlabel{eq:precision}{{2.7}{4}{Metrics}{equation.2.1.7}{}}
\newlabel{eq:recall}{{2.8}{4}{Metrics}{equation.2.1.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Confusion matrix\relax }}{5}{table.caption.4}}
\newlabel{tb:confusion_matrix}{{2.1}{5}{Confusion matrix\relax }{table.caption.4}{}}
\newlabel{eq:f1score}{{2.9}{5}{Metrics}{equation.2.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Data}{5}{subsection.2.1.5}}
\newlabel{se:data}{{2.1.5}{5}{Data}{subsection.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine learning}{5}{section.2.2}}
\newlabel{se:machine_learning}{{2.2}{5}{Machine learning}{section.2.2}{}}
\newlabel{eq:hypotesis}{{2.10}{6}{Machine learning}{equation.2.2.10}{}}
\newlabel{eq:model}{{2.11}{6}{Machine learning}{equation.2.2.11}{}}
\newlabel{eq:error_function}{{2.12}{6}{Machine learning}{equation.2.2.12}{}}
\newlabel{eq:optimization_function}{{2.13}{6}{Machine learning}{equation.2.2.13}{}}
\newlabel{eq:regression_def}{{2.14}{6}{Machine learning}{equation.2.2.14}{}}
\newlabel{eq:classification_def}{{2.15}{6}{Machine learning}{equation.2.2.15}{}}
\citation{srivastava_dropout:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Performance of a model by the number of epochs using training and test set\relax }}{7}{figure.caption.5}}
\newlabel{fig:overfitting}{{2.3}{7}{Performance of a model by the number of epochs using training and test set\relax }{figure.caption.5}{}}
\citation{mcculloch_logical_1943}
\citation{kleene_representation_1951}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Supervised and unsupervised learning}{8}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Artificial neural network (ANN)}{8}{subsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Neuron cell in a brain\relax }}{9}{figure.caption.6}}
\newlabel{fig:neuron}{{2.4}{9}{Neuron cell in a brain\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Artificial neuron cell\relax }}{9}{figure.caption.7}}
\newlabel{fig:ann_neuron}{{2.5}{9}{Artificial neuron cell\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Feed-forward neural network\relax }}{10}{figure.caption.8}}
\newlabel{fig:ann}{{2.6}{10}{Feed-forward neural network\relax }{figure.caption.8}{}}
\newlabel{eq:neuron_net_output}{{2.16}{10}{Artificial neural network (ANN)}{equation.2.2.16}{}}
\newlabel{eq:neuron_output}{{2.17}{10}{Artificial neural network (ANN)}{equation.2.2.17}{}}
\citation{krizhevsky_imagenet_2012,simonyan_very_2014,szegedy_going_2015,he_deep_2016}
\citation{hubel_receptive_1968}
\citation{fukushima_neocognitron:_1982,eckmiller_hierarchical_1989}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep learning}{11}{section.2.3}}
\newlabel{se:deep_learning}{{2.3}{11}{Deep learning}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Convolutional neural network (CNN / ConvNet)}{11}{subsection.2.3.1}}
\citation{krizhevsky_imagenet_2012,simonyan_very_2014,szegedy_going_2015,he_deep_2016}
\citation{simonyan_very_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces 3-dimensional structure of a convolutional filter (on the left painted in pink is the input - image - and the blue rectangle with circles is a filter)\relax }}{12}{figure.caption.9}}
\newlabel{fig:conv}{{2.7}{12}{3-dimensional structure of a convolutional filter (on the left painted in pink is the input - image - and the blue rectangle with circles is a filter)\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A $3 \times 3$ kernel (filter) convolving and input to generate an output\relax }}{13}{figure.caption.10}}
\newlabel{fig:conv1}{{2.8}{13}{A $3 \times 3$ kernel (filter) convolving and input to generate an output\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Representation of filters throughout the CNN\relax }}{13}{figure.caption.11}}
\newlabel{fig:filter}{{2.9}{13}{Representation of filters throughout the CNN\relax }{figure.caption.11}{}}
\citation{simonyan_very_2014}
\citation{srivastava_dropout:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Max pooling a $4\times 4$ input to $2\times 2$\relax }}{14}{figure.caption.12}}
\newlabel{fig:pool}{{2.10}{14}{Max pooling a $4\times 4$ input to $2\times 2$\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical modern CNN\relax }}{14}{figure.caption.13}}
\newlabel{fig:cnn}{{2.11}{14}{Typical modern CNN\relax }{figure.caption.13}{}}
\citation{ioffe_batch_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces An example of the Dropout technique\relax }}{15}{figure.caption.14}}
\newlabel{fig:dropout}{{2.12}{15}{An example of the Dropout technique\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Dropout}{15}{subsection.2.3.2}}
\newlabel{se:dropout}{{2.3.2}{15}{Dropout}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Data augmentation}{15}{subsection.2.3.3}}
\newlabel{se:data_augmentation}{{2.3.3}{15}{Data augmentation}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Backpropagation}{15}{subsection.2.3.4}}
\newlabel{se:backprop}{{2.3.4}{15}{Backpropagation}{subsection.2.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces An example of the original - unprocessed - image in $X_{train}$\relax }}{16}{figure.caption.15}}
\newlabel{fig:data_augmentation_figure}{{2.13}{16}{An example of the original - unprocessed - image in $X_{train}$\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Example of data augmented images based on the original Figure \ref  {fig:data_augmentation_figure}\relax }}{16}{figure.caption.16}}
\newlabel{fig:data_augmentation}{{2.14}{16}{Example of data augmented images based on the original Figure \ref {fig:data_augmentation_figure}\relax }{figure.caption.16}{}}
\citation{kingma_adam:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces An example of local and global maximum and minimum in a function\relax }}{17}{figure.caption.18}}
\newlabel{fig:local_and_global_function_values}{{2.15}{17}{An example of local and global maximum and minimum in a function\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{17}{section*.17}}
\newlabel{eq:gradient_descent}{{2.18}{17}{Gradient descent}{equation.2.3.18}{}}
\citation{bottou-98x}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces An example of gradient descent converging to the global minimum in a 2-dimensional space (function with two variables)\relax }}{18}{figure.caption.19}}
\newlabel{fig:local_and_global_function_values}{{2.16}{18}{An example of gradient descent converging to the global minimum in a 2-dimensional space (function with two variables)\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic gradient descent}{18}{section*.20}}
\newlabel{se:stochastic_gd}{{2.3.4}{18}{Stochastic gradient descent}{section*.20}{}}
\citation{clevert_fast_2015,xu_empirical_2015,he_delving_2015}
\citation{he_deep_2016}
\@writefile{toc}{\contentsline {subsubsection}{Optimization target}{19}{section*.21}}
\newlabel{eq:softmax}{{2.19}{19}{Optimization target}{equation.2.3.19}{}}
\newlabel{eq:cross_entropy_loss}{{2.20}{19}{Optimization target}{equation.2.3.20}{}}
\newlabel{eq:cross_entropy_graidnet}{{2.21}{19}{Optimization target}{equation.2.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Vanishing gradient}{19}{subsection.2.3.5}}
\newlabel{se:vanishing_gradient}{{2.3.5}{19}{Vanishing gradient}{subsection.2.3.5}{}}
\citation{ioffe_batch_2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Batch Normalization}{20}{subsection.2.3.6}}
\newlabel{se:batch_norm}{{2.3.6}{20}{Batch Normalization}{subsection.2.3.6}{}}
\citation{he_deep_2016}
\citation{caffe}
\citation{tensorflow}
\citation{theano}
\citation{keras}
\citation{pytorch}
\citation{cntk}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Model}{21}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{se:model}{{3}{21}{Model}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Implementation}{21}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Keras}{21}{subsection.3.1.1}}
\newlabel{se:keras}{{3.1.1}{21}{Keras}{subsection.3.1.1}{}}
\citation{he_deep_2016,simonyan_very_2014,szegedy_going_2015}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Keras' high-level API example}{22}{lstlisting.3.1}}
\citation{simonyan_very_2014}
\citation{he_deep_2016}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces CNN architecture performance comparison on ImageNet dataset ($error = 1 - accuracy$)\relax }}{23}{table.caption.22}}
\newlabel{tb:CNN_architecutre_performance}{{3.1}{23}{CNN architecture performance comparison on ImageNet dataset ($error = 1 - accuracy$)\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Architecture}{23}{subsection.3.1.2}}
\newlabel{se:architecture}{{3.1.2}{23}{Architecture}{subsection.3.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}ResNet}{23}{subsection.3.1.3}}
\newlabel{se:resnet_architecture}{{3.1.3}{23}{ResNet}{subsection.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The residual layer (module) - the core building block of the ResNet\relax }}{24}{figure.caption.23}}
\newlabel{fig:residual_layer}{{3.1}{24}{The residual layer (module) - the core building block of the ResNet\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Inception v3}{24}{subsection.3.1.4}}
\newlabel{se:inceptionv3_architecture}{{3.1.4}{24}{Inception v3}{subsection.3.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Pretrained parameters}{24}{subsection.3.1.5}}
\newlabel{se:pretrained_parameters}{{3.1.5}{24}{Pretrained parameters}{subsection.3.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A comparison of CNN architectures, from lest to right VGG-19, ResNet-34 without residuals and ResNet-34\relax }}{25}{figure.caption.24}}
\newlabel{fig:vgg_resnet_comparison}{{3.2}{25}{A comparison of CNN architectures, from lest to right VGG-19, ResNet-34 without residuals and ResNet-34\relax }{figure.caption.24}{}}
\citation{imagenet}
\citation{kingma_adam:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Inception v3's Inception module\relax }}{26}{figure.caption.25}}
\newlabel{fig:inception_module}{{3.3}{26}{Inception v3's Inception module\relax }{figure.caption.25}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}Creating a custom pretrained ResNet using the weights from the ImageNet dataset}{26}{lstlisting.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Improving generalization}{27}{subsection.3.1.6}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.3}Data augmentation of the input dataset (images)}{27}{lstlisting.3.3}}
\citation{he_deep_2016}
\citation{imagenet}
\citation{keras}
\citation{clevert_fast_2015}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Final model}{28}{section.3.2}}
\citation{kingma_adam:_2014}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.4}The model that was appended to the end of ResNet-50}{29}{lstlisting.3.4}}
\citation{deep_learning}
\citation{deep_learning}
\citation{imagenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Data}{31}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}ImageNet}{31}{section.4.1}}
\newlabel{se:imagenet}{{4.1}{31}{ImageNet}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Dataset}{31}{subsection.4.1.1}}
\newlabel{se:dataset}{{4.1.1}{31}{Dataset}{subsection.4.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces ImageNet statistics\relax }}{32}{table.caption.27}}
\newlabel{tb:imagenet_statistics}{{4.1}{32}{ImageNet statistics\relax }{table.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Typical image from the ImageNet dataset, from the class herbivore\relax }}{33}{figure.caption.26}}
\newlabel{fig:typical_imagenet_example}{{4.1}{33}{Typical image from the ImageNet dataset, from the class herbivore\relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Dataset statistics\relax }}{33}{table.caption.29}}
\newlabel{tb:dataset_statistics}{{4.2}{33}{Dataset statistics\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Preprocessing}{33}{subsection.4.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Class distribution of training data\relax }}{34}{figure.caption.28}}
\newlabel{fig:training_set_class_distributon}{{4.2}{34}{Class distribution of training data\relax }{figure.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces List of all 15 species used to train the model\relax }}{35}{table.caption.30}}
\newlabel{tb:species_list}{{4.3}{35}{List of all 15 species used to train the model\relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Species}{35}{section.4.2}}
\newlabel{se:species}{{4.2}{35}{Species}{section.4.2}{}}
\newlabel{fig:ursus_thibetanus}{{4.3a}{37}{Ursus thibetanus\relax }{figure.caption.31}{}}
\newlabel{sub@fig:ursus_thibetanus}{{a}{37}{Ursus thibetanus\relax }{figure.caption.31}{}}
\newlabel{fig:sphyrna_tiburo}{{4.3b}{37}{Sphyrna tiburo\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sphyrna_tiburo}{{b}{37}{Sphyrna tiburo\relax }{figure.caption.31}{}}
\newlabel{fig:hippodamia_convergens}{{4.3c}{37}{Hippodamia convergens\relax }{figure.caption.31}{}}
\newlabel{sub@fig:hippodamia_convergens}{{c}{37}{Hippodamia convergens\relax }{figure.caption.31}{}}
\newlabel{fig:ursus_maritimus}{{4.3d}{37}{Ursus maritimus\relax }{figure.caption.31}{}}
\newlabel{sub@fig:ursus_maritimus}{{d}{37}{Ursus maritimus\relax }{figure.caption.31}{}}
\newlabel{fig:vulpes_fulva}{{4.3e}{37}{Vulpes fulva\relax }{figure.caption.31}{}}
\newlabel{sub@fig:vulpes_fulva}{{e}{37}{Vulpes fulva\relax }{figure.caption.31}{}}
\newlabel{fig:cygnus_buccinator}{{4.3f}{37}{Cygnus buccinator\relax }{figure.caption.31}{}}
\newlabel{sub@fig:cygnus_buccinator}{{f}{37}{Cygnus buccinator\relax }{figure.caption.31}{}}
\newlabel{fig:adalia_bipunctata}{{4.3g}{37}{Adalia bipunctata\relax }{figure.caption.31}{}}
\newlabel{sub@fig:adalia_bipunctata}{{g}{37}{Adalia bipunctata\relax }{figure.caption.31}{}}
\newlabel{fig:rodolia_cardinalis}{{4.3h}{37}{Rodolia cardinalis\relax }{figure.caption.31}{}}
\newlabel{sub@fig:rodolia_cardinalis}{{h}{37}{Rodolia cardinalis\relax }{figure.caption.31}{}}
\newlabel{fig:diomedea_exulans}{{4.3i}{37}{Diomedea exulans\relax }{figure.caption.31}{}}
\newlabel{sub@fig:diomedea_exulans}{{i}{37}{Diomedea exulans\relax }{figure.caption.31}{}}
\newlabel{fig:epilachna_varivestis}{{4.3j}{37}{Epilachna varivestis\relax }{figure.caption.31}{}}
\newlabel{sub@fig:epilachna_varivestis}{{j}{37}{Epilachna varivestis\relax }{figure.caption.31}{}}
\newlabel{fig:canis_rufus}{{4.3k}{37}{Canis rufus\relax }{figure.caption.31}{}}
\newlabel{sub@fig:canis_rufus}{{k}{37}{Canis rufus\relax }{figure.caption.31}{}}
\newlabel{fig:triaenodon_obseus}{{4.3l}{37}{Triaenodon obseus\relax }{figure.caption.31}{}}
\newlabel{sub@fig:triaenodon_obseus}{{l}{37}{Triaenodon obseus\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Examples of species with a randomly picked image from the input dataset\relax }}{37}{figure.caption.31}}
\newlabel{fig:specis_image_examples}{{4.3}{37}{Examples of species with a randomly picked image from the input dataset\relax }{figure.caption.31}{}}
\citation{keras}
\citation{tensorflow}
\citation{keras}
\citation{docker}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{38}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{se:results}{{5}{38}{Results}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Hardware and software}{38}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}ResNet-50 and Inception v3 comparison}{38}{section.5.2}}
\newlabel{se:resnet_vs_inception}{{5.2}{38}{ResNet-50 and Inception v3 comparison}{section.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Hardware specifications\relax }}{38}{table.caption.32}}
\newlabel{tb:hardware_specifications}{{5.1}{38}{Hardware specifications\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Software specifications\relax }}{39}{table.caption.33}}
\newlabel{tb:software_specifications}{{5.2}{39}{Software specifications\relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces ResNet-50 and Inception v3 architecture comparison on the validation set\relax }}{39}{table.caption.34}}
\newlabel{tb:resnet_vs_inceptionv3}{{5.3}{39}{ResNet-50 and Inception v3 architecture comparison on the validation set\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Performance}{39}{section.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Software specifications\relax }}{39}{table.caption.37}}
\newlabel{tb:software_specifications}{{5.4}{39}{Software specifications\relax }{table.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces ResNet-50 performance on the training and the validation sets\relax }}{40}{figure.caption.35}}
\newlabel{fig:resnet_performance}{{5.1}{40}{ResNet-50 performance on the training and the validation sets\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Inception v3 performance on the training and the validation sets\relax }}{40}{figure.caption.36}}
\newlabel{fig:inceptionv3_performance}{{5.2}{40}{Inception v3 performance on the training and the validation sets\relax }{figure.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Classifier performance\relax }}{41}{table.caption.38}}
\newlabel{tb:classifier_performance}{{5.5}{41}{Classifier performance\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Memory footprint}{41}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Confusion matrix generated on $X_{validation}$\relax }}{42}{figure.caption.39}}
\newlabel{fig:species_confusion_matrix}{{5.3}{42}{Confusion matrix generated on $X_{validation}$\relax }{figure.caption.39}{}}
\citation{yan_hd-cnn:_2015,yan_hd-cnn:_2015-1}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Model accuracy compared with different number of classes in $X_{validation}$ using the final model\relax }}{43}{figure.caption.40}}
\newlabel{fig:model_acc_class_num}{{5.4}{43}{Model accuracy compared with different number of classes in $X_{validation}$ using the final model\relax }{figure.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Number of parameters\relax }}{43}{table.caption.41}}
\newlabel{tb:model_params}{{5.6}{43}{Number of parameters\relax }{table.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Future work}{43}{section.5.5}}
\newlabel{se:future_work}{{5.5}{43}{Future work}{section.5.5}{}}
\bibstyle{fer}
\bibdata{master_thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{45}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{bottou-98x}{{1}{1998}{{Bottou}}{{}}}
\bibcite{how_many_species_2011}{{2}{2011}{{Camilo~Mora}}{{}}}
\bibcite{clevert_fast_2015}{{3}{2015}{{Clevert et~al.}}{{Clevert, Unterthiner, and Hochreiter}}}
\bibcite{eckmiller_hierarchical_1989}{{4}{1989}{{Fukushima}}{{}}}
\bibcite{fukushima_neocognitron:_1982}{{5}{1982}{{Fukushima and Miyake}}{{}}}
\bibcite{he_delving_2015}{{6}{2015}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{he_deep_2016}{{7}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{46}{chapter*.42}}
\bibcite{caffe}{{8}{}{{http://caffe2.ai/}}{{}}}
\bibcite{theano}{{9}{}{{http://deeplearning.net/software/theano/}}{{}}}
\bibcite{pytorch}{{10}{}{{http://pytorch.org/}}{{}}}
\bibcite{keras}{{11}{}{{https://keras.io/}}{{}}}
\bibcite{cifar}{{12}{}{{https://www.cs.toronto.edu/ kriz/cifar.html}}{{}}}
\bibcite{docker}{{13}{}{{https://www.docker.com/}}{{}}}
\bibcite{kaggle}{{14}{}{{https://www.kaggle.com/}}{{}}}
\bibcite{cntk}{{15}{}{{https://www.microsoft.com/en-us/cognitive toolkit/}}{{}}}
\bibcite{tensorflow}{{16}{}{{https://www.tensorflow.org/}}{{}}}
\bibcite{imagenet}{{17}{}{{http://www.image net.org/}}{{}}}
\bibcite{hubel_receptive_1968}{{18}{1968}{{Hubel and Wiesel}}{{}}}
\bibcite{ioffe_batch_2015}{{19}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{kingma_adam:_2014}{{20}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kleene_representation_1951}{{21}{1951}{{Kleene}}{{}}}
\bibcite{krizhevsky_imagenet_2012}{{22}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{mcculloch_logical_1943}{{23}{1943}{{{McCulloch} and Pitts}}{{}}}
\bibcite{samuel_studies_1959}{{24}{1959}{{Samuel}}{{}}}
\bibcite{simonyan_very_2014}{{25}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{srivastava_dropout:_2014}{{26}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{szegedy_going_2015}{{27}{2015}{{Szegedy et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich}}}
\bibcite{xu_empirical_2015}{{28}{2015}{{Xu et~al.}}{{Xu, Wang, Chen, and Li}}}
\bibcite{yan_hd-cnn:_2015-1}{{29}{2015{a}}{{Yan et~al.}}{{Yan, Jagadeesh, {DeCoste}, Di, and Piramuthu}}}
\bibcite{yan_hd-cnn:_2015}{{30}{2015{b}}{{Yan et~al.}}{{Yan, Zhang, Piramuthu, Jagadeesh, {DeCoste}, Di, and Yu}}}
\bibcite{deep_learning}{{31}{2015}{{Yann~LeCun}}{{}}}
