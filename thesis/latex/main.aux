\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{samuel_studies_1959}
\citation{krizhevsky_imagenet_2012,simonyan_very_2014,szegedy_going_2015,he_deep_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Research context}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definitions and notation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Image representation}{2}}
\newlabel{eq_2d_matrix}{{2.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces RGB image with 4 pixels represented as a 3 dimensional matrix\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:image_matrix}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Gradient}{3}}
\newlabel{eq:gradient}{{2.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Activation functions}{3}}
\newlabel{se:activation_functions}{{2.1.3}{3}}
\newlabel{eq:sigmoid}{{2.3}{3}}
\newlabel{eq:tanh}{{2.4}{3}}
\newlabel{eq:relu}{{2.5}{3}}
\newlabel{fig:sigmoid}{{2.2a}{4}}
\newlabel{sub@fig:sigmoid}{{a}{4}}
\newlabel{fig:tanh}{{2.2b}{4}}
\newlabel{sub@fig:tanh}{{b}{4}}
\newlabel{fig:relu}{{2.2c}{4}}
\newlabel{sub@fig:relu}{{c}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Nonliear activation functions\relax }}{4}}
\newlabel{fig:subfig1.a.4}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Metrics}{4}}
\newlabel{eq:accuracy}{{2.6}{4}}
\newlabel{eq:precision}{{2.7}{4}}
\newlabel{eq:recall}{{2.8}{4}}
\newlabel{eq:f1score}{{2.9}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Confusion matrix\relax }}{5}}
\newlabel{tb:confusion_matrix}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Data}{5}}
\newlabel{se:data}{{2.1.5}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine learning}{5}}
\newlabel{se:machine_learning}{{2.2}{5}}
\newlabel{eq:hypotesis}{{2.10}{5}}
\newlabel{eq:model}{{2.11}{6}}
\newlabel{eq:error_function}{{2.12}{6}}
\newlabel{eq:optimization_function}{{2.13}{6}}
\newlabel{eq:accuracy}{{2.14}{6}}
\newlabel{eq:precision}{{2.15}{6}}
\citation{srivastava_dropout:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Performance of a model with time using training and test set\relax }}{7}}
\newlabel{fig:overfitting}{{2.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Supervised and unsupervised learning}{7}}
\citation{mcculloch_logical_1943}
\citation{kleene_representation_1951}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Neuron cell in a biological brain.\relax }}{8}}
\newlabel{fig:neuron}{{2.4}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Artificial neural network (ANN)}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Artificial neuron cell.\relax }}{9}}
\newlabel{fig:ann_neuron}{{2.5}{9}}
\newlabel{eq:neuron_net_output}{{2.16}{9}}
\newlabel{eq:neuron_output}{{2.17}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Feed-forward neural network\relax }}{10}}
\newlabel{fig:ann}{{2.6}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep learning}{10}}
\citation{hubel_receptive_1968}
\citation{fukushima_neocognitron:_1982,eckmiller_hierarchical_1989}
\citation{krizhevsky_imagenet_2012,simonyan_very_2014,szegedy_going_2015,he_deep_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces 3D structure of a convolutional filter (on the left painted in pink is the input and the blue rectangle with circle is a filter)\relax }}{11}}
\newlabel{fig:conv}{{2.7}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Convolutional Neural Network (CNN / ConvNet)}{11}}
\citation{simonyan_very_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A 3x3 kernel (filter) convolving and input to generate an output\relax }}{12}}
\newlabel{fig:conv1}{{2.8}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Representation of filters through the CNN\relax }}{12}}
\newlabel{fig:filter}{{2.9}{12}}
\citation{simonyan_very_2014}
\citation{srivastava_dropout:_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Max pooling a 4x4 input to 2x2\relax }}{13}}
\newlabel{fig:pool}{{2.10}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical modern CNN\relax }}{13}}
\newlabel{fig:cnn}{{2.11}{13}}
\citation{ioffe_batch_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces An example of the dropout technique\relax }}{14}}
\newlabel{fig:dropout}{{2.12}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Dropout}{14}}
\newlabel{se:dropout}{{2.3.2}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Data Augmentation}{14}}
\newlabel{se:data_augmentation}{{2.3.3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces An example of the original image 224x224x3 in $X_{train}$\relax }}{15}}
\newlabel{fig:data_augmentation_figure}{{2.13}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Example of data augmented images based on the original image \ref  {fig:data_augmentation_figure}\relax }}{15}}
\newlabel{fig:data_augmentation}{{2.14}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Backpropagation}{16}}
\newlabel{se:backprop}{{2.3.4}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Vanishing Gradient}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Batch Normalization}{16}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}TaxNet}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Implementation}{17}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Dataset}{18}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}ImageNet}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{fer}
\bibdata{master_thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{20}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{eckmiller_hierarchical_1989}{{1}{}{{Fukushima}}{{}}}
\bibcite{fukushima_neocognitron:_1982}{{2}{}{{Fukushima i Miyake}}{{}}}
\bibcite{he_deep_2016}{{3}{}{{He et~al.}}{{He, Zhang, Ren, i Sun}}}
\bibcite{hubel_receptive_1968}{{4}{}{{Hubel i Wiesel}}{{}}}
\bibcite{kleene_representation_1951}{{5}{}{{Kleene}}{{}}}
\bibcite{krizhevsky_imagenet_2012}{{6}{}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, i Hinton}}}
\bibcite{mcculloch_logical_1943}{{7}{}{{{McCulloch} i Pitts}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{21}}
\bibcite{samuel_studies_1959}{{8}{}{{Samuel}}{{}}}
\bibcite{simonyan_very_2014}{{9}{}{{Simonyan i Zisserman}}{{}}}
\bibcite{srivastava_dropout:_2014}{{10}{}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, i Salakhutdinov}}}
\bibcite{szegedy_going_2015}{{11}{}{{Szegedy et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, i Rabinovich}}}
\bibcite{fleet_visualizing_2014}{{12}{}{{Zeiler i Fergus}}{{}}}
